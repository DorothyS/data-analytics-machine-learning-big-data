{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook creates and measures a ridge regression model using sklearn.\n",
    "\n",
    "* Method: Ridge Regression\n",
    "* Dataset: Big Mart dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/robert.dempsey/Dev/daamlobd/data/bigmart/big_mart_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "data['Item_Weight'].fillna((data['Item_Weight'].mean()), inplace=True)\n",
    "data['Item_Visibility'] = data['Item_Visibility'].replace(0,np.mean(data['Item_Visibility']))\n",
    "data['Outlet_Establishment_Year'] = 2013 - data['Outlet_Establishment_Year']\n",
    "data['Outlet_Size'].fillna('Small',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables to convert categorical data into numeric values\n",
    "object_cols = list(data.select_dtypes(include=['object']).columns)\n",
    "dummies = pd.get_dummies(data[object_cols], prefix= object_cols)\n",
    "data.drop(object_cols, axis=1, inplace = True)\n",
    "X = pd.concat([data, dummies], axis =1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a Ridge Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into training and cv for cross validation\n",
    "X = X.drop('Item_Outlet_Sales',1)\n",
    "X_train, X_test, Y_train, Y_test = \\\n",
    "    train_test_split(X, data.Item_Outlet_Sales, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of a Ridge Regression model\n",
    "model = Ridge(alpha=0.05, normalize=True)\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intercept Coefficient**: represents the mean change in the response variable for one unit of change in the predictor variable while holding everything else constant. It isolates the role of one variable from all others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the intercept coefficient\n",
    "print('Estimated intercept coefficient: {}'.format(model.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the features and coefficients\n",
    "fc_df = pd.DataFrame(list(zip(X.columns, model.coef_)), columns=['features', 'coefficients'])\n",
    "fc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict a Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot to compare actual sales (Y_test) and the predicted sales (pred_test)\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.scatter(Y_test, y_pred)\n",
    "plt.xlabel(\"Actual Sales: $Y_i$\")\n",
    "plt.ylabel(\"Predicted Sales: $\\hat{Y}_i$\")\n",
    "plt.title(\"Actual vs. Predicted Sales: $Y_i$ vs. $\\hat{Y}_i$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Mean Squared Error (MSE) for all predictions\n",
    "mse = mean_squared_error(Y_train, model.predict(X_train))\n",
    "print(\"MSE Training Data: {}\".format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the MSE for the test data\n",
    "print(\"MSE Test Data: {}\".format(mean_squared_error(Y_test, model.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance (R^2) Score\n",
    "\n",
    "* Explains how much of the variability of a factor can be caused or explained by its relationship to another factor; how well the model is predicting.\n",
    "* A score of 1 means a perfect prediction\n",
    "* A score of 0 means the model always predicts the expected value of y, disregarding the input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Variance Score: %.2f\" % r2_score(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Residuals**: the difference between the predictions and the actuals.\n",
    "\n",
    "\n",
    "**Interpretation**: If the model is working well then the data should be randomly scattered around line zero. If there is structure in the data, that means the model is not capturing something, perhaps interaction between two variables or it's time dependent. Check the parameters of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a residual plot\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.scatter(model.predict(X_train), model.predict(X_train) - Y_train, c='b', s=40, alpha=0.5)\n",
    "plt.scatter(model.predict(X_test), model.predict(X_test) - Y_test, c='g', s=40)\n",
    "plt.hlines(y=0, xmin=0, xmax=50)\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot Using Training (Blue) and Test (Green) Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "The funnel shape indicates Heteroskedasticity. The variance of error terms(residuals) is not constant. Generally, non-constant variance arises in the presence of outliers or extreme leverage values. These values get too much weight, thereby disproportionately influencing the modelâ€™s performance.\n",
    "\n",
    "This indicates signs of non linearity in the data which has not been captured by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Ridge(alpha=0.5, normalize=True)\n",
    "model_2.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Estimated intercept coefficient: {}'.format(model_2.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict a Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_pred = model_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot to compare actual sales (Y_test) and the predicted sales (pred_test)\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.scatter(Y_test, y2_pred)\n",
    "plt.xlabel(\"Actual Sales: $Y_i$\")\n",
    "plt.ylabel(\"Predicted Sales: $\\hat{Y}_i$\")\n",
    "plt.title(\"Actual vs. Predicted Sales: $Y_i$ vs. $\\hat{Y}_i$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Mean Squared Error (MSE) for all predictions\n",
    "mse = mean_squared_error(Y_train, model_2.predict(X_train))\n",
    "print(\"MSE Training Data: {}\".format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the MSE for the test data\n",
    "print(\"MSE Test Data: {}\".format(mean_squared_error(Y_test, model_2.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance (R^2) Score\n",
    "\n",
    "* Explains how much of the variability of a factor can be caused or explained by its relationship to another factor; how well the model is predicting.\n",
    "* A score of 1 means a perfect prediction\n",
    "* A score of 0 means the model always predicts the expected value of y, disregarding the input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Variance Score: %.2f\" % r2_score(Y_test, y2_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Residuals**: the difference between the predictions and the actuals.\n",
    "\n",
    "\n",
    "**Interpretation**: If the model is working well then the data should be randomly scattered around line zero. If there is structure in the data, that means the model is not capturing something, perhaps interaction between two variables or it's time dependent. Check the parameters of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a residual plot\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.scatter(model_2.predict(X_train), model_2.predict(X_train) - Y_train, c='b', s=40, alpha=0.5)\n",
    "plt.scatter(model_2.predict(X_test), model_2.predict(X_test) - Y_test, c='g', s=40)\n",
    "plt.hlines(y=0, xmin=0, xmax=50)\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot Using Training (Blue) and Test (Green) Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to train and measure a logistic regression model with PySpark.\n",
    "\n",
    "* Method: [Logistic Regression](https://spark.apache.org/docs/2.2.0/mllib-linear-methods.html#logistic-regression)\n",
    "* Dataset: Spark MLlib Sample LibSVM Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Some Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkContext and a SQLContext context to use\n",
    "sc = SparkContext(appName=\"Logistic Regression with Spark\")\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = \"/Users/robert.dempsey/Dev/daamlobd/data/mllib/sample_libsvm_data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sqlContext.read.format(\"libsvm\").load(DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View one of the records\n",
    "data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test datasets\n",
    "splits = data.randomSplit([0.8, 0.2], 42)\n",
    "train = splits[0]\n",
    "test = splits[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a Logistic Regression Model\n",
    "\n",
    "Arguments:\n",
    "* maxIter: max number of iterations\n",
    "* regParam: regularization parameter\n",
    "* elasticNetParam: ElasticNet mixing param\n",
    "  * 1 = L1 Regularization (LASSO)\n",
    "  * 0 = L2 Regularization (Ridge)\n",
    "  * Between 0 and 1 = ElasticNet (L1 + L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=10,\n",
    "                        regParam=0.3,\n",
    "                        elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the intercept\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the predictions\n",
    "predictions = lr_model.transform(test)\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the actuals versus predictions\n",
    "actuals = predictions.select('label').collect()\n",
    "predictions = predictions.select('prediction').collect()\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.scatter(actuals, predictions)\n",
    "plt.xlabel(\"Actuals\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.title(\"Actuals vs. Predictions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the summary\n",
    "metrics = lr_model.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area Under ROC\n",
    "\n",
    "A measure of how well a parameter can distinguish between the two groups in a binary classification.\n",
    "\n",
    "* .90-1 = excellent (A)\n",
    "* .80-.90 = good (B)\n",
    "* .70-.80 = fair (C)\n",
    "* .60-.70 = poor (D)\n",
    "* .50-.60 = fail (F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area under the ROC\n",
    "print(\"Area Under ROC = %.2f\" % metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F-Measure (F1)\n",
    "\n",
    "A measure of a test's accuracy that considers both the precision p and the recall r of the test to compute the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all F-Measure scores\n",
    "metrics.fMeasureByThreshold.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the best threshold to maximize the F-Measure\n",
    "f_measure = metrics.fMeasureByThreshold\n",
    "max_f_measure = f_measure.groupBy().max('F-Measure').select('max(F-Measure)').head()\n",
    "best_threshold = f_measure.where(f_measure['F-Measure'] == max_f_measure['max(F-Measure)']) \\\n",
    "    .select('threshold').head()['threshold']\n",
    "print(\"Best Threshold: %0.3f\" % best_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the New Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model using our new threshold\n",
    "lr2 = LogisticRegression(maxIter=10,\n",
    "                         regParam=0.3,\n",
    "                         elasticNetParam=0.8,\n",
    "                         threshold=0.594)\n",
    "# Train the model\n",
    "lrm2 = lr.fit(train)\n",
    "\n",
    "# Create the predictions\n",
    "p2 = lrm2.transform(test)\n",
    "\n",
    "# Plot the actuals vs. predicted\n",
    "a2 = p2.select('label').collect()\n",
    "pred2 = p2.select('prediction').collect()\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.scatter(a2, pred2)\n",
    "plt.xlabel(\"Actuals\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.title(\"Actuals vs. Predictions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New metrics\n",
    "m2 = lrm2.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area under the ROC\n",
    "print(\"Area Under ROC = %.2f\" % m2.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shut it Down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

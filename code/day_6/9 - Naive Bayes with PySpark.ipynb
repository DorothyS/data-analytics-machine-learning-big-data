{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook creates and measures a Naive Bayes classifier with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set SPARK_HOME\n",
    "# environ[\"SPARK_HOME\"] = \"/home/students/spark-2.2.0\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Some Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkContext and a SQLContext context to use\n",
    "sc = SparkContext(appName=\"Naive Bayes Classification with Spark\")\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = \"/Users/robert.dempsey/Dev/daamlobd/data/mllib/sample_libsvm_data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "data = sqlContext.read.format(\"libsvm\").load(DATA_FILE)\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a single row\n",
    "data.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "splits = data.randomSplit([0.6, 0.4], 1234)\n",
    "train = splits[0]\n",
    "test = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of a NaiveBayes model\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "nb_model = nb.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions from the test set\n",
    "predictions = nb_model.transform(test)\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MulticlassClassificationEvaluator\n",
    "\n",
    "The MulticlassClassificationEvaluator expects two input columns: prediction and label.\n",
    "\n",
    "Available metrics:\n",
    "* f1: a measure of a test's accuracy considering both [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall). Best value is 1.\n",
    "  * precision: the fraction of retrieved documents that are relevant to the query\n",
    "  * recall: the fraction of the relevant documents that are successfully retrieved\n",
    "* weightedPrecision\n",
    "* weightedRecall\n",
    "* accuracy: either the fraction (default) or the count (normalize=False) of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the MulticlassClassificationEvaluator to compute accuracy on the test set\n",
    "metrics = ['f1','weightedPrecision','weightedRecall','accuracy']\n",
    "measurements = dict()\n",
    "\n",
    "for metric in metrics:\n",
    "    metric_eval = MulticlassClassificationEvaluator(labelCol=\"label\",\n",
    "                                                    predictionCol=\"prediction\",\n",
    "                                                    metricName=metric).evaluate(predictions)\n",
    "    measurements[metric] = metric_eval\n",
    "\n",
    "for key, value in measurements.items():\n",
    "    print(\"{}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shut it Down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Batch K-Means Clustering with Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to implement Mini Batch K-Means clustering in Sklearn.\n",
    "\n",
    "Mini-batches are subsets of the input data, randomly sampled in each training iteration. These mini-batches drastically reduce the amount of computation required to converge to a local solution. In contrast to other algorithms that reduce the convergence time of k-means, mini-batch k-means produces results that are generally only slightly worse than the standard algorithm.\n",
    "\n",
    "* Method: [Mini Batch K-Means](http://scikit-learn.org/stable/modules/clustering.html#mini-batch-kmeans)\n",
    "* Dataset: Sklearn 20 newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "\n",
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = 10, 8\n",
    "sb.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform analysis on all the categories\n",
    "categories = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some information on the data\n",
    "print(\"%d documents\" % len(data.data))\n",
    "print(\"%d categories\" % len(data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels for the data\n",
    "labels = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the true K of the dataset\n",
    "true_k = np.unique(labels).shape[0]\n",
    "print(\"True K: {}\".format(true_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the features from the training data using a sparse vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of features (dimensions) to extract from the text\n",
    "n_features = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an IDF (Inverse Document Frequency) normalization on the output of HashingVectorizer\n",
    "hasher = HashingVectorizer(n_features=n_features,\n",
    "                           stop_words='english',\n",
    "                           alternate_sign=False,\n",
    "                           norm=None,\n",
    "                           binary=False)\n",
    "vectorizer = make_pipeline(hasher, TfidfTransformer())\n",
    "\n",
    "X = vectorizer.fit_transform(data.data)\n",
    "\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform dimensionality reduction using LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of components used in the dimensionality reduction\n",
    "n_components = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizer results are normalized, which makes KMeans behave as spherical k-means for better results.\n",
    "# Since LSA/SVD results are not normalized, we have to redo the normalization.\n",
    "svd = TruncatedSVD(n_components)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "X = lsa.fit_transform(X)\n",
    "\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a Mini Batch K-Means Clustering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "km = MiniBatchKMeans(n_clusters=true_k,\n",
    "                       init='k-means++',\n",
    "                       n_init=1,\n",
    "                       init_size=1000,\n",
    "                       batch_size=1000,\n",
    "                       verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homogeneity\n",
    "\n",
    "Homogeneity metric of a cluster labeling given a ground truth. A clustering result satisfies homogeneity if all of its clusters contain only data points which are members of a single class.\n",
    "\n",
    "Score between 0 and 1\n",
    "* 1 stands for perfect homogenous (of the same kind) labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completeness\n",
    "\n",
    "A clustering result satisfies completeness if all the data points that are members of a given class are elements of the same cluster\n",
    "\n",
    "Score between 0.0 and 1.0\n",
    "* 1.0 stands for perfectly complete labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V-Measure\n",
    "\n",
    "The harmonic mean between homogeneity and completeness.\n",
    "\n",
    "Score between 0.0 and 1.0.\n",
    "* 1.0 stands for perfectly complete labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted Rand-Index\n",
    "\n",
    "Computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.\n",
    "\n",
    "Similarity score between -1.0 and 1.0.\n",
    "* Random labelings have an ARI close to 0.0.\n",
    "* 1.0 stands for perfect match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Adjusted Rand-Index: %.3f\" % metrics.adjusted_rand_score(labels, km.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Score\n",
    "\n",
    "Shows how well defined the clusters are.\n",
    "\n",
    "Scores\n",
    "* 1: Best\n",
    "* 0: indicates overlapping clusters\n",
    "* -1: Worst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(X, km.labels_, sample_size=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
